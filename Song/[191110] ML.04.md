# [191110] ML. 4 - Domain Adaptation

#### Recommender Systems Overview

- use binary or 평점으로 matrix 생성

- Matrix Factorization (MF)

  - P의 각 row는 각 유저의 latent vector, Q의 각 col은 각 아이템의 Latent vector
  - Element 간의 inner product
  - 기존의 rating matrix 는 아주 sparse하지만 latent vector는 dense 하다.
  - 한계: inner product 매트릭스는 triangle inequality를 만족하지 않는다.
    - similarity propagation process를 반영할 수 없다.
    - Simple triplet loss를 통한 metric learning으로 한계를 극복

  

#### Bridge to domain adaption: SSCDR 소개

Semi-Supervised Learning for Cross-Domain Recommendation to Cold-Start Users

- 강성구 선생님 논문

<b>Cross Domain Recommendation</b>

Recommender system cannot provide personalized recommendations to 새로운 유저

- 그런 유저에게는 그냥 인기순 아이템을 추천하는게 맞을까?

- ㄴㄴ ! 요즘의 유저는 여러 도메인에 소속되어있으니 그 정보를 활용하자!

Source-domain: 정보를 제공하는 다른 도메인

Target-domain: 점수를 매기려는 도메인

Overlapping users: 두 도메인의 정보를 모두 갖고 있는 유저

- <b>Goal: 새로운 유저(cold start users)에게 top-N 추천을 제공한다</b>



[기존 연구]

1. Embedding and Mapping Approach(2017)

   1. MF를 통해 유저와 아이템에 대한 매트릭스를 두 도메인에 대해 각각 학습

   2. 소스 데이터 매트릭스로 타겟 데이터 매트릭스를 예측하는 신경망 학습

      - This a supervised linear regression

   3. Recommending items to cold-start users

      

   가정: Source 도메인에서 겹치는 유저들이 대체로 Target 도메인에서도 겹친다

   결론: 군집이 겹치면 그 가중치만 배울 수 있으면 새로운 유저에 대해 예측할 수 있다.

   한계: Mapping function only uses <b>the supervised loss, performance is highly sensitive</b>

   - 증명: <Challenge: Overlapping users are highly limited

- 기존 연구들의 또 다른 한계
  - INNER PRODUCT는 triangle inequality를 반영 못함
  - 기존 연구는 euclidean distance로 mapping function defined



[선생님의 연구]

Motivation: 극한의 적은 교집합 교차 도메인을 어떻게 학습할 것인가

Our Key Idea

- Formulate training of cross domain mapping function as a <b>semi supervised task</b>
  - Local consistency assumption of semi supervised learning
  - Fundamental assumption of CDR



#### Semi Supervised learning for Cross-Domain Recommendation (SSCDR)

(1) collaborative filtering in metric space

- Learn user and item latent vectors for each domain in metric spaces

- "user-user similarity" as well as user-item similarity

  

- <b> Semi-Supervise Learning</b>

  - Overlapping users 가 적을 때도 어떻게 좋은 성능을 발휘하게 만들까

  - Iterating을 하면서 label데이터와 같은 label을 가질 확률이 높은 유저에게 labeling

  - Semi supervised Metric space mapping

    - 기존 overlapping 데이터를 통한 supervise loss (+)

    - triplet loss by unsupervised labeled data (unsupervised loss)

      = 오버래핑 유저가 소스 도메인에서 다른 유저들과의 유사도를 기반으로,

      타겟 도메인에서의 아이템 latent vector를 학습한다.

(3) Multi-hop neighborhood inference

​	Provide recommendations to cold start users by using neighborhood information

1. Aggregate neighborhood information
2. Make recommendations



#### Domain Adaption

- Unsupervised Domain Adaptation by Back propagation(15)

Task: Unsupervised domain adaptation

- Lots of data in the source domain which is fully labeled
- Lots of data in the target domain
  - which is partially labeled - semi supervised
  - which is fully unlabeled - unsupervised

- Target domain의 label을 얻는것이 힘들어서 적합한 mapping function을 학습하기 힘들다

- KEY IDEA

  - Upper Bound on the Target risk(06)
    - source risk + domain divergence
  - Focus on learning features that are 
    - (1) Discriminative and 
    - (2) Domain-invariant = 두 도메인에 대한 shift를 무한하게 학습한다

- Proposed method

  - feature extractor
  - label predictor
  - domain classifier = 1 or 0 데이터가 소스인지 타겟인지

  <b>Gradient reversal layer</b>(domain 분류기)

  = 로스의 부호를 바꾼다  = 도메인이 1 인지 0 인지 구별하지 못하도록 학습된다



