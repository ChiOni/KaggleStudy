# [191201] ML.07_1

#### Learning from scarce data

Meta-learning: learning to learn

- Aim to train a model that can rapidly adapt to a new task
- 적은 양의 데이터로부터, 새로운 task를 빠르게 학습할 수 있는 Meta Learner를 학습
- Few-Shot Learning 학습을 잘 하기 위한 모델을 학습



< Paper 1>

### Learning to Learn: Model Regression Networks for Easy Small Sample Learning(2016) - Yu-Xiong Wang

#### Motivation

"Learning novel categories from few examples" are still challenging

- Robots are supposed to recognize unfamiliar objects
- Humans can, the model can



#### The Central Issue

How to estimate a classifier learned from <b>a large set of samples</b> based on

its corresponding classifier learned from <b>few annotated samples</b>

- 관통 가설 
  - there exists a generic, category agnostic <b>transformation T</b>
    - from classifiers w(0) 적은 샘플로 학습된
    - to the underlying classifiers w(*) 많은 샘플로 학습된 
  - Intuitively, a model can be viewed as a separating hyperplane in the feature space
  - with small training examples, the model can find <b>initial hyperplane</b>
  - when gradually adding new examples, hyper plane be the <b>optimal hyper plane</b>
    - Transformation T는 완벽히 같은 Task에 대해서 따로따로 학습해줘야한다

#### Proposed Method

- Model Regression Network
  - a deep regression network on a large collection of model pairs
  - initial parameter -> optimal parameter
  - 다양한 카테고리의 이분류 문제들을 학습한다면 T를 찾을 수 있다!

- part 1: 파라미터 거리 최소

- part 2: 주어진 few-shot task 잘 수행

  

- For novel categories with few instances
  - initialization
  - transformation
    - obtain the output model T(w(0), theta) 
  - refinement
    - learn w by minimizing R(w)
      - part 1: Transformation한 파라미터와 유사하게
      - part 2: 주어진 few task를 잘 수행하게

#### Experimental Results

-  모델 transformation > target only > fine tuning > source & target > source only



<paper 2>

### Learning to Model the Tail(2017) - Yu-Xiong Wang

#### Motivation

Extends the previous work to effectively learn few-shot models 

for classes in the tail of the class distribution

- Instead of learning a single fixed meta-learner for all the k-shot tasks,

  it learns a <b> series of transformation</b>

== transformation T를 학습하는건 동일한데 모든 k-shot task에 대해서 각각 T를 학습해야된다



#### Key Idea

- Model parameters across different classes share similar model dynamics
- Goal: 모델의 학습 과정 자체를 학습해서 knowledge가 부족한 task에 적용한다

중간에 parameter가 optimal해지면 그 이후로는 identical한 학습이 된다

- F(i) denotes the meta-learner that transforms k-shot theta(0) to theta(*)

