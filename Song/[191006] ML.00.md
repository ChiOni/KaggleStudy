#### [191006] ML.0

2주간은 기본적인 머신러닝 개념 복기 및 PyTorch 연습



- Microsrt Azure Machine Learning: Algorithm Cheat Sheat
  - 많은 경우(데이터가 충분하지 않은 경우) 기존 머신러닝 알고리즘이 더 좋은 효율을 낸다.



- 기본 예제: House Price 예측
  - Train Instances (N x K)
  - Train Labels (N x 1)
    - 비-머신 러닝: Rule Base
    - 머신 러닝: 각 feature 별 '가중치' 를 학습
      1. 가중치의 임의 초기화
      2. 초기값을 통한 예측, 실제값과의 차이를 계산
      3. Gradien Descent를 통해 최적 가중치에 도달



#### MACHINE LEARNING 핵심 개념

- Generalization

  = 한번도 보지 않은 데이터에서도 좋은 결과를 얻는 것

  

  - Techiniuqe 1: Regularizaiton

    - Overfitting을 막는 기법들을 통칭

    - Weight Decay (L2 regularization) 파라미터의 크기를 제한하는 기법

      

  - Techinique 2: Train, Test, Valid Set

    - Train set의 일부를 학습에 사용하지 않고, valid set으로 사용한다.

    - Valid set에서 가장 성능이 좋은 모델을 test에 사용한다.

    - 마지막 epoch이 아닌 최적 valid parameter로 설정해줘야 한다.

      

  - Techinique 3: Epoch, Batch, Mini-Batch, Stochastic Learning

    - Batch Size에 따른 학습 방법이 차이

    - 일반적인 경우 size가 클수록 높은 성능을 보인다(but, high cost)

      

  - Techinique 4: Supervised, Unsupervised, Semi-supervised Learning

    - Label 존재 유무에 따라 구분
    - Supervised
      - Classification  (Discrete) - 최신 트렌드 == Semi supervised 분류
      - Regression
    - Unsupervised
      - Feature Selection
        - 모든 데이터를 넣는 것보다 축소한 데이터를 넣는게 더 효율이 좋을 수 있다.
        - Concrete Autoencoders, ICDM(2019) - In MNIST Data
        - 정답이 없음으로 Unsupervised
      - Clustering



#### Machine Learning 분야의 연구 주제들

- Natural Language Processing Group

  - Machine Translation / Text summarization

    

- Computer Vision Group

  - Visual Question Answering (VQA) - 이미지를 통한 상황 유추 (supervised)

  - Generation Image Description - 이미지 설명

    

- Recommender System Group

  - Cross-Domain Recommendation - 멜론 음악 데이터를 기반으로 카카오 페이지 책 추천

  - Rating Elicitation - 사전 설문을 어떻게 구성해야 가장 잘 특성을 뽑을 수 있을까

    

- Anomaly Detection Group



#### 선생님 하셨던 연구들

- Social Network Based Recommendation (KDBC 17)
  - 각 개인마다 가장 영향을 많이 끼친 사람을 찾아 추천에 활용
- Fake News Detection (IMCOM 20)
  - 각 news는 여러 개의 component들로 구성된다.
  - 가짜 뉴스를 판단하기 위해서는 component의 특성 뿐 아나리 관계 역시 파악해야한다.
- Semi-Supervised Cross-Domain Recommendation (CIKM 19)
  - 교집합의 사용자만 사용해서 cross domain 추천을 사용하여 무가치하다.
  - Semi-Supervised하게 접근
- Rating Elicitation
  - 딥러닝 기반 rating elicitation
- Semi-Supervised Anomaly Detection
  - 정상 데이터가 unimodal일 때는 잘 작동하지만, 여러 개의 modality를 갖는 상황을 해결



#### Linear Regression, Logistic Regression 모델

- Linear Regression
  - Loss function ( MSE = L2, MAE = L1)
- Logistic Regression
  - sigmoid( linear model ) = Output이 확률의 의미를 갖게 만든다.
  - Loss function  ( Cross-Entropy )



#### Neural Network Basic

- Perceptron: Single-Layer Neural Network
  - Activation function f: binary/sigmoid/Rectified Linear Unit(ReLU)
  - Backpropagation by chain rule
    - No feature learning - 없는 feature를 학습하지 못한다.
- Multilayer Perceptron (MLP)
  - Nonlinear classfication - feature learning
- Widely Used Loss Functions
  - Cross entropy loss
  - Mean squared error (L2 loss)



#### Practical Tips

- Network Optimization
  - Learning rate
- Issues in Deep Neural Networks
  - Large amount of training time
  - Overfitting
  - Vanishing gradient problem - using ReLU
- Stochastic Gradient Descent
- Overfitting - weight decay
- 선생님 Tips
  1. 모델이 train set을 overfitting 할 수 있는지 확인한다.
  2. Learning rate는 그래프를 보고 '적절히' 조절한다.
  3. 각 weight의 크기: 한번에 update되는 크기는 1000:1 정도가 적당하다.



#### PyTorch 실습

- PyTorch - Facebook group
- Tensorflow 별로다 요즘
- .to(cpu or gpu) 텐서 연산을 어디서 할거냐?

