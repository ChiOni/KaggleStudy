# [191201] ML.07_2

#### Adversarial Attack

- Attempts to fool models through malicious input

- Adversarial examples

  - inputs to machine learning models that an attacker has <b>intentionally designed</b>

    to cause the model to <b>make a mistake</b>

  - People <b>cannot ditinguish</b> the difference from the original clean image



<paper 1>

#### Explaining and Harnessing Adversarial Examples (2015)

- Main Concept
  - Standard training
    - Ground Truth를 잘 학습하도록 <b>모델을 학습</b>
  - Adversarial Attack
    - 모델이 정답을 못 맞추도록 <b>이미지를 수정</b>



- Linear models and adversarial examples
  - Let's first consider a simple linear model (W) and input data (x)
  - We now add a small noise (z) to input data, x' = x + z
  - Then, the output of the model is
    - W^t * x' = W^t *x + W^t * z
      - the activation of the network increases by W^t * z
      - the increase can be maximized by z = epsilon * sign(W)



- Fast Gradient Sign Method (FGSM)
  - Let's consider a neural network (theta) that have cost function J
  - 가정: neural network는 별로 non-linear하지 않다



#### How to Defend?

- Gradient Masking (Failed) - practical black-box attacks against machine learning
  - trying to deny the attacker access to a useful gradient
  - 그냥 답만 주자, 퍼센트를 주지 말고

- Adversarial Training (So So)
  - 학습과정에서 adversarial example도 함께 학습한다



