# 02. Case studies

#### Why look at case studies?

- 프로그래밍 배울 때, 다른 사람 코드 읽는 거랑 똑같다!

  

#### Classic networks:

#### 1. LeNet-5

- Input = 32 x 32 x 1
- Filter = 5 x 5, stride = 1
- Output1 = 28 x 28 x 6 (6: #filters)
- Avg pool = 14 x 14 x 6 (filter size = 2, stride = 2)
- Filter = 5 x 5, stride = 1
- ...

(컨볼루션 네트워크 -> Average 풀링) x 2 + Flatten 작업을 통한 2번의 Fully Connected layer

n_h, n_w는 점점 감소하고 n_c( #channel)은 점점 증가한다.

- sigmoid/tanh > relu in classic nn
- 채널의 파라미터를 줄이기 위해, 각 채널이 바라보고 있는 인풋의 크기를 다르게 조정
- non-linearity of pooling layer



#### 2. AlexNet

Input = 227 x 227 x 3

- Stride를 4로 크게 잡아 차원의 크기를 많이 줄인다.
- (# Filter)는 96 -> Max pooling

![image1](C:\Users\littl\kaggle\Song\deeplearning.ai\Lecture4\images\image1.JPG)

- Multiple GPU

- Local Response Normalization

  

#### 3. VGG - 16

- 많은 하이퍼파라미터를 가졌지만, 구성은 단순한 네트워크
- Conv = 3 x 3 filter, stride = 1, same
- Max-Pool = 2 x 2, stride = 2



#### ResNets

- Learn skip layer
- What is residual block?
  - a[l] -> a[l+2]로 가는 길에, a[l+1]을 linear operate하고 relu를 적용하기 전에 a[l]을 더해준다.
  - 앞에 위치한 데이터의 정보를 뒤로 효율적으로 전달할 수 있다.
  - short cut or skip connection으로 부른다.
- Neural Net이 점점 깊어질수록, input의 정보가 너무 손실된다.
- Short cut을 통해 깊은 신경망에서도 input의 정보를 전달한다.
- short cut을 적용할 데이터와 output은 같은 차원이어야 한다.

![image2](C:\Users\littl\kaggle\Song\deeplearning.ai\Lecture4\images\image2.JPG)



#### Networks in Networks and 1x1 Convolutions

(EX) 6x6x32 input에 대해 1x1x32 convolution을 적용하여 single output을 얻을 수 있다.

Conv net을 통해 Fully connect net을 가능하게 한다. (Network in Network)

= 적은 parameter로 fully connect의 기능을 수행할 수 있다.

- pooling은 height & width를 줄인다. 그렇다면 채널은 어떻게 줄일 수 있을까?
  - 1x1 convolution의 필터수를 [l-1] 시점의 channel보다 줄이면 얻을 수 있다.



#### Inception Network

다양한 size의 filter를 stacking하여 기존 input보다 깊은 output layer를 만들 수 있다.

ex) 28x28x192 input을 1x1,3x3,5x5 same filter들로 stacking한다.

![image3](C:\Users\littl\kaggle\Song\deeplearning.ai\Lecture4\images\image3.JPG)

