# 05. Deep Neural Networks

- Forward propagation in a deep network
- Getting your matrix dimensions right
  - Dimension of W[l] = (n[l],n[l-1])

ex) input -> 5,5,3 -> output deep NN

X: (n[0],m)

W1:(n[1],n[0])

b1:(n[1],m)

Z1:(n[1],m)

A1:(n[1],m)



W2:(n[2],n[1])

b2:(n[2],m)

Z2:(n[2],m)

A2:(n[2],m)

...

Z[l],A[l] : (n[l],m)

dZ[l],dA[l] : (n[l],m)



#### Why deep representations?

get feature -> component, so deep layer can express complex things

This procedure can use not only image, but also audio



- Circuit theory and deep learning

  = You can express complex functions with 'small' 'deep' NN, but when using shallow NN, you need exponentially large units



#### Building blocks of deep neural networks

- Forward and backward functions

Layer l: W[l],b[l]

Forward: Input = a[l-1], output = a[l]

Backward: Input = da[l], output = da[l-1]



#### Forward and Backward Propagation

![img](https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/3EXZkrZDEemCfwoJPcV7XA_8ba14e9c50511530d512da45f9037ac4_Screen-Shot-2019-08-04-at-6.09.44-AM.png?expiry=1566172800000&hmac=2OVCkVU-beYNl-9i4pDhUWPhCyyngJUFRR2wJjmk-og)













