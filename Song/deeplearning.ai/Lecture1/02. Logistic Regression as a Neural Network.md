# 02. Logistic Regression as a Neural Network

#### Binary Classification

- Logistic regression is an algorithm for binary classification

- input: Image => output: Cat or Non cat

  - red,blue,green의 64 x 64 x 3 dimension의 input feature
  - x = [64 x 64 x 3]  =>  y = [0 or 1]

- Notation

  - (x,y)  x is in R^(Nx) , y is in [0,1]

  - m training examples: {(x1,y1),(x2,y2)...}

  - X = [x1,x2,x3...xm] is the (m x Nx) size matrix

    ```python
    X.shape == (m,Nx)
    ```

  - Y = [y1,y2,...ym]

    ```python
    Y.shape ==(1,m)
    ```



#### Logistic Regression

Given x, want y^ such that y^ is the probability P(y=1|x) when x is in R^(Nx)

Parameters: w is in R^(Nx),b

Output y^ = W(transpose)X + b



sigmoid(z) = 1/(1+e^(-z))

If z large the sigmoid(z) ~= 1

If z very negative large the sigmoid(z) ~= 0



#### Logistic Regression Cost Function

L(y^,y) = -(  ylog(y^) + (1-y)log(1-y^)  )

If y=1: L = -logy^

If y=0: L = -log(1-y^)

Cost function: J(w,b) = 1/m * sum(L)



#### Gradient Descent

y^ = sigmoid(W(transpose)x + b)

J(w,b) = 1/m * sum(L)

Want to find w,b that minimize J(w,b)

When the cost function is convex, we have global minimul

1. Initialize the w,b
2. Takes a step in the steepest downhill direction
3. Converge to this global optimum

w := w - a*(dJ/dw)

a = learning rate

dJ/dw = dw = derivate J of w

so we can write it as w:= w- adw



#### Computation Graph

Computation graph comes in handy when there is some distinguished or some special output variable

Ex) J(a,b,c) = 3(a + bc)

​      let u = bc, v = a + u, then J = 3v

```
a						 -->  v =  a +u  --> J =3v

b -->	u = bc	-->

c -->  
```

Ex) dJ/db = dJ/du * du/db -- chain rule



#### Logistic Regression Gradient Descent

![1565612664847](C:\Users\littl\AppData\Roaming\Typora\typora-user-images\1565612664847.png)

Let two features x1,x2  and parameters w1,w2,b

Then z =w1x1 + w2x2  + b ==> a = sigmoid(z) ==> L(a,y)

Let da = dL/da

​			= -y/a + (1-y)/(1-a)  we can get this form when we derivate sigmoid ftn

Let dz = dL/dz = dL/da * da/dz

​			= (-y/a + (1-y)/(1-a)) * (a(1-a)) = a-y

Let dw1 = dL/dw1 = x1 * dz  // dw2 = x2*dw  // db = dz

Then w1 := w1 - a*dw1

​		w2 := w2 - a*dw2

​		b := b - a*db

















