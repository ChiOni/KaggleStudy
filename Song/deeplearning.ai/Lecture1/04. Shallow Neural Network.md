# 04. Shallow Neural Network

#### Neural Networks Overview

#### <Logistic Regression>

x

w	-	z=w.t*x+b	-	a=sigmoid(z)	-	L(a,y)

b 

#### <Neural net>

x1	-		neuron 1

x2	-		neuron 2	-	neuron	-	y.pred

x3	-		neuron 3

<input> <layer1>	<layer2>	<output>

Same way in back propagation



#### Neural Network Representation

Input layer --  hidden layer -- output layer

- Input Layer = a[0] = X
- Hidden Layer = a[1] = [a11,a12,...a14] when 4 nodes exist ~~ w[1],b[1]
- Output Layer = a[2] = Y(pred)

When only one hidden layer exist, the neural network is 2 Layer NN(Don't count input layer)



#### Computing a Neural Network's Output

![1565846117527](C:\Users\littl\AppData\Roaming\Typora\typora-user-images\1565846117527.png)



#### Vectorizing across multiple examples

m개의 트레이닝 샘플을 가로로 쌓고 n개의 노드를 세로로 쌓아서 n X m 의 Z와 A를 얻자!



#### Activation functions

- sigmoid 	1/(1+e^-z)

- hyper tangent     (e^z-e^(-z))/(e^z+e^(-z))

  those two activation have issue when x is too big or small

  == because the derivate of activation function close to 0

- ReLU     max(0,z) makes learning faster

  실제로 z값이 정확히 0일 가능성이 거의 없어 미분가능성에 대해서는 고민하지 않아도 된다

- leaky ReLU     max(0.01*z,z)



#### Why do you need non-linear activation functions?

when the activation function in identity(Linear) function:

then a1 = z1 = w[1]x +b[1]

then a2 = w(2)(w[1]x+b[1]) + b[2] ... it's just same as one linear function

== we don't need 'deep' model when the activation function is linear

== non-linear activation function can lead model as non-linear



#### Derivatives of activation functions

sigmoid = 1/(1+e^-z)

d/dz(sigmoid) = sigmoid*(1-sigmoid)



tanh = (e^z - e^-z) / (e^z + e^-z)

d/dz(tanh) = 1- (tanh)^2



ReLU = max(0,z)

d/dz(ReLU) = 0 if z<0 and 1 if z>0 and undefined when z=0



#### Gradient descent for neural networks

- Forward propagation

Z1 = W1X + b1

A1 = g1(Z1)

Z2 = W2A1 + b2

A2 = g2(Z2)

- Back propagation

dZ2 = A2 - Y

dW2 = 1/m * dz2(A1.T)

db2 = 1/m * np.sum(dz2,axis = 1, keepdims = True)

dZ1 = (W2.T)dz2 * d/dz(g1)(Z1)



#### Back propagation intuition

![1566016341464](C:\Users\littl\AppData\Roaming\Typora\typora-user-images\1566016341464.png)



#### Random Initialization

When we initialize W & b all zero, then all neuron have same function

w = np.random.rand((2,2)) * 0.01

b = np.zero((2,1))  when there are two units in input layer



  