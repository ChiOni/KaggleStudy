# 02. Setting up your optimization problem

#### Normalizing training sets

- subtract mean

- normalize variance

  ​	x = x - mean

  ​	x = x/variance

Training sets and test sets 같은 normalizing method 사용해야 한다

x1  & x2가 매운 다른 범위를 가지고 있으면 w1과 w2가 더 큰 값에 의존적으로 설정된다



#### Vanishing/Exploding gradients

![1566724189954](C:\Users\littl\AppData\Roaming\Typora\typora-user-images\1566724189954.png)

In case we use ReLu activation function,

If all elements of W  > 0,  then Z[l]의 값이 점점 커져 깊은 신경망에서 Y의 값이 exploding

If all elements of W < 1, then Z[l]의 값이 점점 작아져 깊은 신경망에서 Y의 값이 vanishing



#### Weight Initialization for Deep Networks

Better or Careful choice for initialize weights

- In single neuron

  - z = w1x1 + w2x2 + ... + wNxN

  - In large n -> smaller w

  - Var(w) = 2/n

  - w = np.random.rand(shape) * np.sqrt(2/n)

    = W가 1에 수렴하도록 도와주는 method

- Other variant

  - np.sqrt(1/n[l-1])
  - np.sqrt(2/ (n[l-1]+n[l])  )



#### Numerical approximation of gradients

Build up for gradient checking

= checking your derivative computation

= f(a+epsilon) - f(a-epsilon) /  2*epsilon is similar with g(a)

= two-side derivative checking is much slower than one-side but it's accurate



#### Gradient checking

= use it to debug

Take W,b  as theta

for each theta:

​	derivate of theta(i) = cost(a + epsilon) - cost(a - epsilon) / 2*epsilon

​	check euclidean distance between two of them

​	[ derivate of theta - derivate of cost]**2 / 각 값의 제곱의 합

​	이 값이 굉장히 작다면 각 theta에 대한 components의 값이 타당하다



#### Gradient checking implementation Notes

- Don't use in training - only to debug

- If algorithm fails grad check, look at components to try to identify bug

- Remember regularization

- Doesn't work with dropout
- Run at random initialization; perhaps again after some training

