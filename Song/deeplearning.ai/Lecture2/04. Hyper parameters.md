# 04. Hyper parameters

- learning rate: alpha

- momentum: beta

- #layers

- #hidden units

- learning rate decay

- mini-batch size

How to optimize multiple hyper parameters?

- Try random values: Don't use a grid
  - 어떤 변수가 모델에서 가장 중요할지 미리 알 수 없다.
- Coarse to fine



#### Appropriate scale for hyper parameters

특정 범위의 값들에 대한 hyper parameters의 최적화를 진행할 때,

log scale하게 parameter의 값을 옮겨가는 것이 옳다.

EX) 0.001 ~ 1을 0.1단위로 보는 것은 0.001~0.1까지의 값들에 가혹하다.

 따라서 0.001 - 0.01 - 0.1 - 1 식으로 구간 단위를 설정하여 보는 것이 옳다.

- Hyper parameters for exponentially weighted averages
  - beta = 0.9 ~ 0.999
  - beta로 인해 가중 평균에 사용되는 시간의 scale을 고려하여 optimize를 진행한다.



#### Hyper parameters tuning in practice: Pandas vs Caviar

- Intuitions do get stale
- Re-evaluate occasionally

