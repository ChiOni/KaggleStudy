# 05. Batch Normalization

#### Normalizing activation in a network

- we already normalize inputs to speed up learning

  - It makes learn w,b more efficiently

    

Can we normalize a[2] so w[3],b[3] learn more faster?

- Implementing Batch Norm

1. Given some intermediate value in NN: z[i]
2. Get the mean of z[i] = mue
3. then get std such that std = 1/m*np.sum(z[i]-mue)**2
4. Get z[i].norm = z[i]-mue  / root(std**2 + epsilon)
5. Normalized z[i] = gamma*z[i].norm + beta



#### Fitting Batch Norm into a neural network

![1567297641743](C:\Users\littl\AppData\Roaming\Typora\typora-user-images\1567297641743.png)

- In tensorflow, we can imply this without details

- And we can apply this procedure into mini-batches

  

  ![1567297899548](C:\Users\littl\AppData\Roaming\Typora\typora-user-images\1567297899548.png)

- 정규화 단계에서 beta가 b의 역할을 대신하므로 b를 학습단계에서 생략한다.

- Implementing gradient descent

  ```python
  for t=1 ... num(MiniBatches):
      compute foraward propagation on X[t]
      	In each hidden layer, use BN to z[l] with z[l].norm
      Use back propagation to compute dw[l],dbeta[l],dgamma[l]
      update parameters w[l] = w[l] - alpha*dw[l] ...
  ```

  - work with adam or RMSprop



#### Why does Batch Norm Work?

Intuition: 

- 정규화는 학습을 빠르게 만든다.
- Learning on shifting input distribution
  - "Covariate shift": X- > Y의 학습한 것이, X가 변해도 적용되야한다.
  - 새로운 TEST SET의 분포가 모델의 성능에 영향을 미치지 않도록 만든다.
  - 각 층이 앞 단에 영향을 받지 않고 독립적으로 학습할 수 있는 환경을 만든다.

- Batch Norm as regularization
  - Each mini-batch has more noise than entire data set.
  - Scaling has a slight regularization effect. 



#### Batch norm at test time

- Using moving average

