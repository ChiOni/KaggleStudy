# 01. Regularizing your neural network

#### Regularization

![1566645400259](C:\Users\littl\AppData\Roaming\Typora\typora-user-images\1566645400259.png)

- One way to penalize complexity, would be to add all weights to our loss function
- But this method might result in our loss getting so huge that the best model would be to set all weights to 0
- To prevent that, we multiply the sum of squares with another smaller number. And this number is called weight decay



#### Why regularization reduces over fitting

One piece of intuition = 

모델은 로스를 줄이고 싶어서 weight를 0으로 만들고 싶어진다=

모델의 비선형성, 즉 복잡성을 줄이는 방향으로 간다=

Simpler network == reduces over fitting



Second piece of intuition=

In case, g(z)= tanh(z), W를 줄이면 Z가 줄어들고, 활성화 함수의 미분값이 선형성을 띈다 =

모델이 선형성을 띈다 = Over fitting을 벗어날 수 있다



#### Dropout Regularization

- Implementing dropout("Inverted dropout")

EX) illustrate layer l=3 & keep-prob = 0.8

```python
import numpy as np
keep-prob = 0.8
d3 = np.random.rand(a3.shape[0],a3.shape[1]) < keep-prob #True or False
a3 = np.multiply(a3,d3)
a3 /= keep-prob #dropout 비율로 나누어줌으로서 z의 기댓값을 유지한다
```



#### Understanding Dropout

Intuition: Can't rely on any one feature, so have to spread out weights

= Shrink weights(Similar effect with L2 regularization)



#### Other regularization methods

- Data augmentation = fake train examples

  - horizontal reverse
  - randomly zoom image 
  - distortion

  Of course, Adding new data is a better choice

- Early stopping

  1. Plot both dev set error & traing error
  2. Find the low peak of dev set error
  3. stop iteration 

- 