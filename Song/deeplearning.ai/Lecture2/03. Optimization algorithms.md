# 03. Optimization algorithms

#### Mini-batch gradient descent

Batch  (vs)  mini-batch gradient descent

- Vectorization allows you to efficiently compute on m examples
- 너무 많은 데이터가 있을 때, 더 빨리 미니멈을 찾아가는 방법

![1567215555454](C:\Users\littl\AppData\Roaming\Typora\typora-user-images\1567215555454.png)



#### Exponentially weighted (moving) averages

- When we want to get local average

v(given day) = 0.9 * v(previous day) + 0.1*(today temperature)

![1567215913493](C:\Users\littl\AppData\Roaming\Typora\typora-user-images\1567215913493.png)

V(t) is approximately average over 1/(1-beta) days

- High value of beta:
  - more smoother
  - Adapts more slowly, So, there's just a bit more latency



#### Understanding exponentially weighted moving averages

![1567216480188](C:\Users\littl\AppData\Roaming\Typora\typora-user-images\1567216480188.png)

(1-beta) 부분을 제외하고 (beta)**(days)가 1/e 가 되는 시점을 moving average가 영향을 미치는 기간이라고 생각하면 된다.

#### Bias correction in exponentially weighted averages

![1567217065923](C:\Users\littl\AppData\Roaming\Typora\typora-user-images\1567217065923.png)

beta가 클 때, 추정의 초반 너무 낮은 평균치를 제공하는 경향이 있다.

따라서 초반 부분에는 (1-beta**t)를 통해 가중해주는 것이 좋다.



#### Gradient descent with momentum

```python
momentum:
    on iteration t:
        compute dw,db on current mini-batch
        Vdw = beta(Vdw) + (1-beta)dw
        Vdb = beta(Vdb) + (1-beta)db
        W = W-alpha(Vdw), b = b - alpha(Vdb)
```

- 2차원의 학습을 직관적으로 생각하면:
  - 만약 수직적인 학습 방향이 위,아래를 번갈아간다면 과거 값으로 상쇄시킨다.
  - 수평적인 학습이 한 쪽 방향으로만 이루어진자면 과거 값으로 가중시킨다.
  - 따라서 수직적으로는 slow, 수평적으로는 faster한 학습을 돕는다.



#### RMSprop

- root mean square propagation

![1567218208074](C:\Users\littl\AppData\Roaming\Typora\typora-user-images\1567218208074.png)

- 직관적으로 가로축과 세로축의 학습이 존재할 때,
  - 가로는 dw, 세로는 db에 영향을 받는다고 생각하자.
  - 무의미하게 변동이 큰 세로축의 학습의 경우, 제곱을 통해 Sdb가 큰 값이 된다.
  - 그것을 역전파과정에서 분모로 넣어주기 때문에 세로축으로 학습을 억제한다.



#### Adam optimization algorithm

- 모멘텀 + RMSprop

```python
initialize Vdw=0,Sdw=0,Vdb=0,Sdb=0
On interation t:
    Compute dw,db using current mini-batch
    Vdw = beta1(Vdw) + (1-beta1)dw, Vdb = beta1(Vdb) + (1-beta1)db
    Sdw = beta2(Sdw) + (1-beta2)dw**2, Sdb = beta2(Sdb) + (1-beta2)db**2
    Vdw(correct) = Vdw/(1-beta1**t), Vdb(correct) = Vdb/(1-beta1**t)
    Sdw(correct) = Sdw/(1-beta2**t), Sdb(correct) = Sdb/(1-beta2**t)
    
    W = W - alpha* Vdw(correct)/ (root(Sdw(correct)) + epsilon)
    b = b - alpha* Vdb(correct)/ (root(Sdb(correct)) + epsilon)
```

Hyper parameter:

- alpha: needs to be tune
- beta 1: 0.9 /  beta 2: 0.999 (Adam paper recommend)
- epsilon: 10**(-8) 

Adam: adaptive moment estimation



#### Learning rate decay

1 epoch = 1 pass through data ( All mini-batch )

alpha = 1 / ( 1 + decay rate*epoch-number ) * alpha_zero

- Learning rate를 점점 줄여나감에 따라 mini-batch의 노이즈와 최소값을 찾지 못하는 문제를 해결할 수 있다.



(+) Other learning rate decay methods



exponentially learning rate decay:  0.95**(epoch-number) * alpha_zero

or   k/root(epoch_number) * alpha_zero

or   discrete stair case



#### The problem of local optima

- 국소 최적에 대한 문제와 최적화 방법

![1567219476723](C:\Users\littl\AppData\Roaming\Typora\typora-user-images\1567219476723.png)







