# [191013] ML.1 

#### Intro to Convolutional Neural Network

- Multi layer Perceptron
- Fully Connected Layer

- 타겟이 이미지의 어디에 있는지가 성능에 큰 영향을 끼쳤다.

  - 아이디어1: 슬라이딩 윈도우(Sliding Window)
  - 아이디어2: 더 많은 데이터와 더 깊은 신경망
    - Data Augmentation

  

- CNN의 동작 방식

  - Step 1: 이미지를 중첩된 이미지 타일들로 나누기 
  - Step 2: 각 이미지 타일을 작은 신경망에 제공(feed) 하기
  - Step 3: Pooling - 시료 채취하기
    - Max-pooling
  - Step 4: Fully Connected Layer -> Prediction
  - 반복 - 반복 - 반복

  반복할 수록, 점점 더 high-level의 feature들을 학습하게 된다.



- Convolution: locally weighted sum

  - 이미지 타일들 간에 공유되는 <b>커널</b>
  - If kernel dim = Input dim: Fully Connected Layer

- Pooling operations

  - 커널을 적용하고 나서, 배열의 크기를 줄이기 위해서 사용하는 기법
    - Max pooling
    - Average pooling
  - Hyper Parameter: size of grid & size of stride

- Layer 가 깊어질수록, 더 넓의 이미지, 더 중요한 정보를 갖고 있는다.

- Reason for failure

  - Insufficient training data
  - Slow convergence
    - Vanishing gradient problem: Sigmoid Function
    - Too many parameters
    - Limited computing resources
  - Lack of theory: needed to rely on trials-and-errors
    - 수학적 기반 설명 없이 결과에 대한 관측만 가능하다.

  -> 극복!



#### More Formal descriptions of CNN

- Given an image and a filter
  - 필터의 깊이는 항상 convolution 연산 대상의 깊이와 같다.
  - EX) Image dim 32 x 32 x 3 -> 5 x 5 x <b>3</b>
  - Inner product를 통해 얻는 result + bias
- Padding
  - Convolution 연산을 통해 줄어드는 이미지의 크기를 조절
    - 성능이 zero padding이 일반적으로 높다.
    - Filter size에 따라 zero pad size 결정
- Multiple Filters
  - 같은 크기의 filter를 여러 개 사용하여 multiple activation maps 사용



```PYTHON
class Conv_net(nn.Module):
    def __init__(self, num_classes):
        super(Conv_net, self), __init__()
        self.layer1 = nn.Sequential(
        	nn.Conv2d(in_channels = 1, out_channels = 16,
                     kernel_size = 5, stride = 1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size = 2, stride = 2)
        )
        
        self.layer2 = nn.Sequential(
        	nn.Conv2d(in_channels = 16,out_channels = 32,
                     kernel_size = 3, stride = 1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size = 2, stride = 2)
        )
        self.fc = nn.Linear(5*5*32, num_classes)
        # torch.Size([1,32,5,5]) 배치사이즈,채널,세로,가로
        
    def forward(self, x):
        out = self.layer1(x)
        out = self.layer2(out)
        out = out.view(x,size(0),-1) # reshape와 동일
        out = self.fc(out)
        return out

# size구하기
model = Conv_Net(num_classes)
x,y = train_dataset.__getitem__(5)
tmp = model.layer1(x,reshape((1,1,28,28)))
tmp = model.layer2(tmp)
tmp.shape
```



#### LeNet( LeCun et al., 1998)

- Lenet5(1998)
- Conv filters were 5 x 5, applied at stride 1
- Pooling layers were 2 x 2 applied at stride 2
  - 일반적으로 layer가 깊어질수록, 필터의 크기는 줄이고 채널은 키운다



#### VGGNet

- Kernel size는 3으로 고정하고, dim만 커진다.
  - 64 -> 128 -> 256 -> 512
- fully connected layer을 3층으로



#### GoogLeNet

- Inception module
  - 어떤 FILTER가 가장 좋을지 모르니깐, 한 layer에서 여러개를 시도해보고 결과 종합
- Auxiliary classifier
  - Vanishing gradient 해결 방법
    - 중간 중간에도 Gradient를 만든다
- VGG에 비해 훨씬 가벼운 모델, 준수한 성능
- 훨씬 어렵고 복잡하여 V2,V3,V4를 계속해서 발표하고 있다



#### ResNet

아주 단순하게 Layer를 굉장히 깊게 쌓는다.

그러나, 무작정 쌓기만 한다고 성능이 오르지 않는다.

모델이 커졌음에도 불구하고, train error마저 높아졌다.

Vanishing gradient 등의 문제로, 모델이 너무 깊으면 학습시키기가 어렵다.

- Residual learning
  - Main idea: make it easy to learn the identity mapping!
    - H(x) = F(x) + x	
      - Shortcut / residual connection
    - 학습 parameter를 늘리지 않고 성능을 높이는 방법
    - 오히려 너무 high level feature는 분류를 방해한다.
    - 더해줄때는 사이즈를 맞춰주는 과정이 필요하다.
  - y = x + F(x)



#### Transfer Learning

Motivation 

- layer들이 쌓일 수록, 모델의 kernel들은 더 high-level의 feature들을 capture
- 따라서 하위 layer는 그냥 공유해도 될 것 같다.

(Freezing Layer) + 추가로 붙여주는 Layer

```python
resnet = model.resnet18(pretrained = True)
num_features = resnet.fc.in_features
resnet.fc = nn.Linear(num_features, num_classes)
```

- 앞의 CONV NET은 FREEZE하고 FULLY CONNECTED LAYER만 학습한다!









